2023-02-06 01:11:14.716127: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-02-06 01:11:15.379373: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64
2023-02-06 01:11:15.379441: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64
2023-02-06 01:11:15.379451: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-02-06 01:11:36.330992: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64
2023-02-06 01:11:36.331031: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)
2023-02-06 01:11:36.331050: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:163] no NVIDIA GPU device is present: /dev/nvidia0 does not exist
2023-02-06 01:11:36.331280: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-02-06 01:11:36.360477: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled
dev_v2.py:74: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  df = df.append({**results_dict, **params}, ignore_index=True)
EXPERIMENT 0
TRAINING
Epoch 0
   training loss 0.03918152675032616, (0.037747197, 228.0637, 14.25301, 0.9030918)
   validation loss 0.03537014126777649, (0.034221314, 506.09613, 11.397954, 0.9030918)
decoder loss ratio: 0.180162, decoder SINDy loss  ratio: 1.000619
Epoch 100
   training loss 6.487951759481803e-05, (2.6167429e-05, 1.7807144, 0.24505103, 1.4206985)
   validation loss 6.283143011387438e-05, (1.742794e-05, 0.49327338, 0.31196505, 1.4206985)
decoder loss ratio: 0.000092, decoder SINDy loss  ratio: 0.027387
Epoch 200
   training loss 3.370503327460028e-05, (1.0302745e-05, 0.9698426, 0.09410976, 1.3991314)
   validation loss 3.3084270398830995e-05, (1.1335684e-05, 0.18362892, 0.07757274, 1.3991314)
decoder loss ratio: 0.000060, decoder SINDy loss  ratio: 0.006810
Epoch 300
   training loss 3.202942025382072e-05, (1.3514355e-05, 0.80048496, 0.057620876, 1.275298)
   validation loss 3.228422428946942e-05, (1.5310608e-05, 0.12589696, 0.04220638, 1.275298)
decoder loss ratio: 0.000081, decoder SINDy loss  ratio: 0.003705
Epoch 400
   training loss 2.4243403458967805e-05, (8.448571e-06, 0.60612446, 0.040720694, 1.1722763)
   validation loss 2.1597799786832184e-05, (7.093609e-06, 0.08820857, 0.027814291, 1.1722763)
decoder loss ratio: 0.000037, decoder SINDy loss  ratio: 0.002442
Epoch 500
   training loss 1.7841352018876933e-05, (3.4701166e-06, 0.5892746, 0.032343585, 1.1136878)
   validation loss 1.845615952333901e-05, (4.9864752e-06, 0.076292574, 0.023328066, 1.1136878)
decoder loss ratio: 0.000026, decoder SINDy loss  ratio: 0.002048
THRESHOLDING: 39 active coefficients
Epoch 600
   training loss 1.8655475287232548e-05, (4.8044667e-06, 0.59656423, 0.030527975, 1.0798211)
   validation loss 1.752412208588794e-05, (4.8133074e-06, 0.07025969, 0.019126033, 1.0798211)
decoder loss ratio: 0.000025, decoder SINDy loss  ratio: 0.001679
Epoch 700
   training loss 1.892463114927523e-05, (5.8175447e-06, 0.40435147, 0.025958566, 1.051123)
   validation loss 1.8015354726230726e-05, (5.7456577e-06, 0.06564134, 0.017584667, 1.051123)
decoder loss ratio: 0.000030, decoder SINDy loss  ratio: 0.001544
Epoch 800
   training loss 1.6104921087389812e-05, (3.5409657e-06, 0.35448158, 0.022261184, 1.0337838)
   validation loss 1.569377855048515e-05, (3.7692937e-06, 0.06074171, 0.015866477, 1.0337838)
decoder loss ratio: 0.000020, decoder SINDy loss  ratio: 0.001393
Epoch 900
   training loss 2.023857450694777e-05, (8.223777e-06, 0.26377895, 0.018320791, 1.0182718)
   validation loss 2.117999247275293e-05, (9.45703e-06, 0.05714523, 0.015402443, 1.0182718)
decoder loss ratio: 0.000050, decoder SINDy loss  ratio: 0.001352
Epoch 1000
   training loss 1.370366499031661e-05, (2.0415869e-06, 0.21677755, 0.016071059, 1.0054972)
   validation loss 1.3951714208815247e-05, (2.4488234e-06, 0.05339964, 0.014479188, 1.0054972)
decoder loss ratio: 0.000013, decoder SINDy loss  ratio: 0.001271
THRESHOLDING: 30 active coefficients
REFINEMENT
Epoch 0
   training loss 2.851034423656529e-06, (1.1510233e-06, 0.2295332, 0.017000113, 1.006418)
   validation loss 2.736206852205214e-06, (1.2970081e-06, 0.05367051, 0.014391988, 1.006418)
decoder loss ratio: 0.000007, decoder SINDy loss  ratio: 0.001263
Epoch 100
   training loss 2.8269751055631787e-06, (1.4318898e-06, 0.22820742, 0.013950853, 1.081352)
   validation loss 2.797860588543699e-06, (1.7780039e-06, 0.03292016, 0.010198567, 1.081352)
decoder loss ratio: 0.000009, decoder SINDy loss  ratio: 0.000895
Epoch 200
   training loss 4.110821919312002e-06, (2.8732993e-06, 0.22690028, 0.012375225, 1.1160537)
   validation loss 4.678986442741007e-06, (3.6762488e-06, 0.032169186, 0.010027378, 1.1160537)
decoder loss ratio: 0.000019, decoder SINDy loss  ratio: 0.000880
Epoch 300
   training loss 3.2999760151142254e-06, (2.2441727e-06, 0.25073484, 0.010558034, 1.1383156)
   validation loss 3.968369583162712e-06, (3.1109187e-06, 0.026999187, 0.00857451, 1.1383156)
decoder loss ratio: 0.000016, decoder SINDy loss  ratio: 0.000753
Epoch 400
   training loss 7.877856660343241e-06, (6.7893516e-06, 0.23197438, 0.010885049, 1.1573197)
   validation loss 1.0436006050440483e-05, (9.31727e-06, 0.033788834, 0.01118736, 1.1573197)
decoder loss ratio: 0.000049, decoder SINDy loss  ratio: 0.000982
Epoch 500
   training loss 1.2908229109598324e-05, (1.1535638e-05, 0.21091516, 0.013725914, 1.1744356)
   validation loss 1.1745425581466407e-05, (1.0600468e-05, 0.03138957, 0.011449579, 1.1744356)
decoder loss ratio: 0.000056, decoder SINDy loss  ratio: 0.001005
Epoch 600
   training loss 1.9009636162081733e-05, (1.7521272e-05, 0.21336122, 0.014883648, 1.1880376)
   validation loss 2.4172904886654578e-05, (2.2425309e-05, 0.0367422, 0.017475951, 1.1880376)
decoder loss ratio: 0.000118, decoder SINDy loss  ratio: 0.001534
Epoch 700
   training loss 7.481974989786977e-06, (6.517611e-06, 0.12371781, 0.009643642, 1.1998593)
   validation loss 8.264685675385408e-06, (7.212874e-06, 0.03308221, 0.010518117, 1.1998593)
decoder loss ratio: 0.000038, decoder SINDy loss  ratio: 0.000923
Epoch 800
   training loss 1.3208703421696555e-05, (1.2060794e-05, 0.11617682, 0.011479098, 1.2097315)
   validation loss 1.2304086340009235e-05, (1.11098525e-05, 0.0314019, 0.011942343, 1.2097315)
decoder loss ratio: 0.000058, decoder SINDy loss  ratio: 0.001048
Epoch 900
   training loss 3.3319568046863424e-06, (2.6196697e-06, 0.07690627, 0.0071228715, 1.2194833)
   validation loss 2.8228876089997357e-06, (2.339354e-06, 0.024730604, 0.0048353355, 1.2194833)
decoder loss ratio: 0.000012, decoder SINDy loss  ratio: 0.000424
Epoch 1000
   training loss 1.4836406990070827e-05, (1.3813654e-05, 0.13388899, 0.010227531, 1.2277293)
   validation loss 1.3346100786293391e-05, (1.25103e-05, 0.028446954, 0.008358008, 1.2277293)
decoder loss ratio: 0.000066, decoder SINDy loss  ratio: 0.000734
/src/data/aesindy_dump/experiment_results_202302060256.pkl
